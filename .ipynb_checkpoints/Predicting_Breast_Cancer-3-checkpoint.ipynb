{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ec242204",
   "metadata": {},
   "source": [
    "Sophie, Mahak and Anna's Final Project: Predicting Breast Cancer Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61556a5e",
   "metadata": {},
   "source": [
    "## 1. first download the data from the internet, save ERS information as CSVs \n",
    "\n",
    "   ### 1a. (keep excel versions though to document variable names)\n",
    "\n",
    "## 2. load the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8910770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d481742",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/sophie/Desktop/CS_data/late_stage_by_county.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wv/36f_9pbd263dsclbtwf6kq8r0000gn/T/ipykernel_40653/4149393873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlate_stage_by_county0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/sophie/Desktop/CS_data/late_stage_by_county.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/sophie/Desktop/CS_data/late_stage_by_county.csv'"
     ]
    }
   ],
   "source": [
    "late_stage_by_county0 = pd.read_csv(\"/Users/sophie/Desktop/CS_data/late_stage_by_county.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "education0 = pd.read_csv(\"/Users/sophie/Desktop/CS_data/Education.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf284dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty0 = pd.read_csv(\"/Users/sophie/Desktop/CS_data/poverty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98461266",
   "metadata": {},
   "outputs": [],
   "source": [
    "population0 = pd.read_csv(\"/Users/sophie/Desktop/CS_data/PopulationEstimates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e757ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_median_income0 = pd.read_csv(\"/Users/sophie/Desktop/CS_data/unemployment_median_income.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1c715",
   "metadata": {},
   "source": [
    "## 3. clean the data:\n",
    "\n",
    "### breast cancer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting with the breast cancer data:\n",
    "\n",
    "#late_stage_by_county0.head(15)\n",
    "\n",
    "#drop extra rows at top:\n",
    "late_stage_by_county1 = late_stage_by_county0.iloc[8:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef367d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make column names the headers:\n",
    "late_stage_by_county1.columns = late_stage_by_county0.iloc[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make row names the row names:\n",
    "late_stage_by_county1.set_index(\"State\", inplace = True)\n",
    "#late_stage_by_county1.drop('State', axis=1)\n",
    "# print(late_stage_by_county1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8971b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out columns we don't want data from\n",
    "col_names = []\n",
    "for col in late_stage_by_county1.columns:\n",
    "    col_names.append(col)\n",
    "#print(col_names)\n",
    "\n",
    "late_stage_by_county1.drop(col_names[0], axis=1, inplace=True) #FIPS\n",
    "late_stage_by_county1.drop(col_names[1], axis=1, inplace=True) #Met Healthy People Objective\n",
    "late_stage_by_county1.drop(col_names[3], axis=1, inplace=True) #Lower CI\n",
    "late_stage_by_county1.drop(col_names[4], axis=1, inplace=True) #Upper CI\n",
    "late_stage_by_county1.drop(col_names[5], axis=1, inplace=True) #CI Rank\n",
    "late_stage_by_county1.drop(col_names[6], axis=1, inplace=True) #Lower CI Rank\n",
    "late_stage_by_county1.drop(col_names[7], axis=1, inplace=True) #Upper CI Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6089fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop NaN's\n",
    "late_stage_by_county1 = late_stage_by_county1.replace('*', np.nan) #to take out N/A values as documented by NIH\n",
    "late_stage_by_county1 = late_stage_by_county1.replace('data not available', np.nan)\n",
    "late_stage_by_county1 = late_stage_by_county1.dropna()\n",
    "#late_stage_by_county1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80490be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#late_stage_by_county1.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect list of counties to pull from other data:\n",
    "counties_included = []\n",
    "for place_name in late_stage_by_county1.index:\n",
    "    if \"County\" in place_name:\n",
    "        counties_included.append(place_name[:-3]) #removes number in parentheses from NIH documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3198387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(counties_included)\n",
    "late_stage_by_county1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa01421",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_county_names = []\n",
    "state_names = []\n",
    "\n",
    "for place in late_stage_by_county1.index:\n",
    "    split_name = place.split(\", \")\n",
    "    cleaned_county_names.append(split_name[0])\n",
    "    if len(split_name) > 1:\n",
    "        state = split_name[1]\n",
    "        state_names.append(state[:-3])\n",
    "    else:\n",
    "        state_names.append(np.nan)\n",
    "    \n",
    "print(cleaned_county_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4399d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CITE: https://gist.github.com/rogerallen/1583593\n",
    "\n",
    "us_state_to_abbrev = {\n",
    "    \"Alabama\": \"AL\",\n",
    "    \"Alaska\": \"AK\",\n",
    "    \"Arizona\": \"AZ\",\n",
    "    \"Arkansas\": \"AR\",\n",
    "    \"California\": \"CA\",\n",
    "    \"Colorado\": \"CO\",\n",
    "    \"Connecticut\": \"CT\",\n",
    "    \"Delaware\": \"DE\",\n",
    "    \"Florida\": \"FL\",\n",
    "    \"Georgia\": \"GA\",\n",
    "    \"Hawaii\": \"HI\",\n",
    "    \"Idaho\": \"ID\",\n",
    "    \"Illinois\": \"IL\",\n",
    "    \"Indiana\": \"IN\",\n",
    "    \"Iowa\": \"IA\",\n",
    "    \"Kansas\": \"KS\",\n",
    "    \"Kentucky\": \"KY\",\n",
    "    \"Louisiana\": \"LA\",\n",
    "    \"Maine\": \"ME\",\n",
    "    \"Maryland\": \"MD\",\n",
    "    \"Massachusetts\": \"MA\",\n",
    "    \"Michigan\": \"MI\",\n",
    "    \"Minnesota\": \"MN\",\n",
    "    \"Mississippi\": \"MS\",\n",
    "    \"Missouri\": \"MO\",\n",
    "    \"Montana\": \"MT\",\n",
    "    \"Nebraska\": \"NE\",\n",
    "    \"Nevada\": \"NV\",\n",
    "    \"New Hampshire\": \"NH\",\n",
    "    \"New Jersey\": \"NJ\",\n",
    "    \"New Mexico\": \"NM\",\n",
    "    \"New York\": \"NY\",\n",
    "    \"North Carolina\": \"NC\",\n",
    "    \"North Dakota\": \"ND\",\n",
    "    \"Ohio\": \"OH\",\n",
    "    \"Oklahoma\": \"OK\",\n",
    "    \"Oregon\": \"OR\",\n",
    "    \"Pennsylvania\": \"PA\",\n",
    "    \"Rhode Island\": \"RI\",\n",
    "    \"South Carolina\": \"SC\",\n",
    "    \"South Dakota\": \"SD\",\n",
    "    \"Tennessee\": \"TN\",\n",
    "    \"Texas\": \"TX\",\n",
    "    \"Utah\": \"UT\",\n",
    "    \"Vermont\": \"VT\",\n",
    "    \"Virginia\": \"VA\",\n",
    "    \"Washington\": \"WA\",\n",
    "    \"West Virginia\": \"WV\",\n",
    "    \"Wisconsin\": \"WI\",\n",
    "    \"Wyoming\": \"WY\",\n",
    "    \"District of Columbia\": \"DC\",\n",
    "    \"American Samoa\": \"AS\",\n",
    "    \"Guam\": \"GU\",\n",
    "    \"Northern Mariana Islands\": \"MP\",\n",
    "    \"Puerto Rico\": \"PR\",\n",
    "    \"United States Minor Outlying Islands\": \"UM\",\n",
    "    \"U.S. Virgin Islands\": \"VI\",\n",
    "    np.nan : \"US\"\n",
    "}\n",
    "\n",
    "state_abrvs = []\n",
    "for state in state_names:\n",
    "    abrv = us_state_to_abbrev[state]\n",
    "    state_abrvs.append(abrv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_stage_by_county1[\"County Name\"]= cleaned_county_names\n",
    "late_stage_by_county1.set_index(\"County Name\", inplace = True)\n",
    "late_stage_by_county1[\"State\"] = state_abrvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85717e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_stage_by_county1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4841ffc",
   "metadata": {},
   "source": [
    "## woohoo! NIH data is clean! now for affluence data... \n",
    "\n",
    "\n",
    "### population data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "population0.head(15)\n",
    "\n",
    "population1 = population0.iloc[1:-3, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dfab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make column names the headers:\n",
    "population1.columns = population0.iloc[0, :8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make row names the row names:\n",
    "population1.set_index(\"Area name\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f55c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take out columns we don't want data from\n",
    "col_names = []\n",
    "for col in population1.columns:\n",
    "    col_names.append(col)\n",
    "#print(col_names)\n",
    "\n",
    "population1.drop(col_names[0], axis=1, inplace=True) #FIPS\n",
    "population1.drop(col_names[2], axis=1, inplace=True) #Rural-urban continuum code 2013\n",
    "population1.drop(col_names[3], axis=1, inplace=True) #1990 Pop\n",
    "population1.drop(col_names[4], axis=1, inplace=True) #2000 Pop\n",
    "population1.drop(col_names[5], axis=1, inplace=True) #2010 Pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3335c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "population1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b46fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "population1.to_csv(\"/Users/Sophie/Desktop/pop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fff7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_without_state_names = []\n",
    "for county in counties_included:\n",
    "    split_name = county.split(\", \")\n",
    "    counties_without_state_names.append(split_name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "population1.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d49d6f7",
   "metadata": {},
   "source": [
    "### poverty data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c91c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty0.head(15)\n",
    "\n",
    "poverty1 = poverty0.iloc[4:, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00864ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make column names the headers:\n",
    "poverty1.columns = poverty0.iloc[3, :9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make row names the row names:\n",
    "poverty1.set_index(\"Area_name\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_names = poverty1.Stabr\n",
    "state_names.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty1 = poverty1.POVALL_2019\n",
    "poverty1 = poverty1.to_frame()\n",
    "poverty1[\"State\"] = state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12081573",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495947d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "poverty1.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883efd55",
   "metadata": {},
   "source": [
    "### education:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "education0.head(15)\n",
    "\n",
    "education1 = education0.iloc[4:, :]\n",
    "education1.columns = education0.iloc[3, :]\n",
    "education1.set_index(\"Area name\", inplace = True)\n",
    "\n",
    "education1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_col_names = []\n",
    "for col in education1.columns:\n",
    "    edu_col_names.append(col)\n",
    "\n",
    "education1 = education1.drop(columns = edu_col_names[2:-4])\n",
    "education1 = education1.drop(columns = \"FIPS Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "education1 = education1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4025e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "education1.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fe9ad",
   "metadata": {},
   "source": [
    "### median household income and unemployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_median_income0.head(15)\n",
    "\n",
    "unemployment_median_income1 = unemployment_median_income0.iloc[4:, :]\n",
    "unemployment_median_income1.columns = unemployment_median_income0.iloc[3, :]\n",
    "unemployment_median_income1.set_index(\"Area_name\", inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f108ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemp_median_income_col_names = []\n",
    "for col in unemployment_median_income1.columns:\n",
    "    unemp_median_income_col_names.append(col)\n",
    "    \n",
    "#print(unemp_median_income_col_names)\n",
    "\n",
    "unemployment_median_income1 = unemployment_median_income1.drop(columns = unemp_median_income_col_names[2:-7])\n",
    "unemployment_median_income1 = unemployment_median_income1.drop(columns = \"FIPS_Code\")\n",
    "unemployment_median_income1 = unemployment_median_income1.drop(columns = unemp_median_income_col_names[-1])\n",
    "unemployment_median_income1 = unemployment_median_income1.drop(columns = unemployment_median_income1.columns[2:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac985122",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_county_names = []\n",
    "state_names = []\n",
    "\n",
    "for place in unemployment_median_income1.index:\n",
    "    split_name = place.split(\", \")\n",
    "    cleaned_county_names.append(split_name[0])\n",
    "    if len(split_name) > 1:\n",
    "        state = split_name[1]\n",
    "        state_names.append(state[:-3])\n",
    "    else:\n",
    "        state_names.append(np.nan)\n",
    "    \n",
    "# print(cleaned_county_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b163d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_median_income1[\"County Name\"]= cleaned_county_names\n",
    "unemployment_median_income1.set_index(\"County Name\", inplace = True)\n",
    "unemployment_median_income1[\"State Name\"] = state_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_median_income1 = unemployment_median_income1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a12faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_median_income1.drop(columns = \"State Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a643f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unemployment_median_income1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fc655",
   "metadata": {},
   "source": [
    "## Now lets compile a big data set, with only counties that are included across all 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c6cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_locations(df):\n",
    "    states = df.State\n",
    "    locs = []\n",
    "    for i in range(len(states)):\n",
    "        locs.append((df.index[i], states[i]))\n",
    "    df[\"County_State\"] = locs\n",
    "    return locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a865b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_locs = set(list_of_locations(late_stage_by_county1))\n",
    "population_locs = set(list_of_locations(population1))\n",
    "education_locs = set(list_of_locations(education1))\n",
    "poverty_locs = set(list_of_locations(poverty1))\n",
    "unemp_median_income_locs = set(list_of_locations(unemployment_median_income1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_included_locs = breast_cancer_locs.intersection(population_locs)\n",
    "all_included_locs = all_included_locs.intersection(education_locs)\n",
    "all_included_locs = all_included_locs.intersection(poverty_locs)\n",
    "all_included_locs = all_included_locs.intersection(unemp_median_income_locs)\n",
    "\n",
    "#print(len(all_included_locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b176b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through each df and drop any rows that aren't included in the compiled list\n",
    "#make a new column in each df with NaN for any county not in all 5\n",
    "\n",
    "def NA_columns(df):\n",
    "    new_col = []\n",
    "    for tup in df.County_State:\n",
    "        if tup in all_included_locs:\n",
    "            new_col.append(0)\n",
    "        else:\n",
    "            new_col.append(np.nan)\n",
    "    df[\"Included\"] = new_col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f7270f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NA_columns(late_stage_by_county1)\n",
    "NA_columns(population1)\n",
    "NA_columns(education1)\n",
    "NA_columns(poverty1)\n",
    "NA_columns(unemployment_median_income1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1293d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "late_stage_by_county1 = late_stage_by_county1.dropna()\n",
    "population1 = population1.dropna()\n",
    "education1 = education1.dropna()\n",
    "poverty1 = poverty1.dropna()\n",
    "unemployment_median_income1 = unemployment_median_income1.dropna()\n",
    "\n",
    "#late_stage_by_county1.sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a31b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.merge(late_stage_by_county1, population1, on = \"County_State\")\n",
    "all_data = pd.merge(all_data, education1, on = \"County_State\")\n",
    "all_data = pd.merge(all_data, poverty1, on = \"County_State\")\n",
    "all_data = pd.merge(all_data, unemployment_median_income1, on = \"County_State\")\n",
    "all_data = all_data.drop(columns = [\"Included_x\",\"Included_y\", \"State_x\", \"State_y\", \"Included\", \"State Name\", \"State\" ])\n",
    "\n",
    "all_data.set_index(\"County_State\", inplace = True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f16824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot age adjusted incidence rate to see how theyre distributed- read lit to see whats high vs. low\n",
    "## once we have cutoffs, create target array to feed into model\n",
    "## start w linear- email lucian once we have results to figure out optimal modeling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339d479",
   "metadata": {},
   "source": [
    "## WOOHOO! Data Set is cleaned and compiled!\n",
    "\n",
    "### now... we need to run a correlation on the data to ensure we will see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc93159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_strings(col_name):\n",
    "    new_col = []\n",
    "    for num in all_data[col_name]:\n",
    "        new_col.append(float(num.replace(',', '')))\n",
    "    all_data[col_name] = new_col\n",
    "\n",
    "# replace_strings(\"Population 2020\")\n",
    "# replace_strings(\"POVALL_2019\")\n",
    "# replace_strings(\"Median_Household_Income_2019\")\n",
    "\n",
    "for col in all_data.columns:\n",
    "    replace_strings(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb436efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr_matrix = all_data.corr()\n",
    "#print(corr_matrix)\n",
    "\n",
    "plt.figure(figsize = (10,16))\n",
    "sn.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25872f8",
   "metadata": {},
   "source": [
    "## Now to apply a multivariate linear regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812eb5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9da9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the other attributes from the predicting attribute\n",
    "x = all_data.drop(\"Average Annual Count\",axis=1)\n",
    "x = x.drop(\"Age-Adjusted Incidence Rate([rate note]) - cases per 100,000\", axis = 1) #dropping extra predictive variables\n",
    "x = x.drop(\"Percent of Cases with Late Stage\", axis = 1)\n",
    "\n",
    "y = all_data[\"Age-Adjusted Incidence Rate([rate note]) - cases per 100,000\"]\n",
    "y = y.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc587a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# creating an object of LinearRegression class\n",
    "LR = LinearRegression()\n",
    "\n",
    "# fitting the training data\n",
    "LR.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction =  LR.predict(x_test)\n",
    "y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing r2_score module\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# predicting the accuracy score\n",
    "score=r2_score(y_test,y_prediction)\n",
    "print(\"r2 socre is\",score)\n",
    "print(\"mean_sqrd_error is\",mean_squared_error(y_test,y_prediction))\n",
    "print(\"root_mean_squared error of is \",np.sqrt(mean_squared_error(y_test,y_prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98fbb0a",
   "metadata": {},
   "source": [
    "### This R^2 value is not promising... lets try something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b31bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_counts = all_data[\"Average Annual Count\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0339f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(annual_counts, range(0, len(annual_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_count = np.mean(np.array(annual_counts))\n",
    "median_count = np.median(np.array(annual_counts))\n",
    "\n",
    "print(average_count)\n",
    "print(median_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71322a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_low =[]\n",
    "for count in annual_counts:\n",
    "    if count >= median_count:\n",
    "        high_low.append(1) #1 indicates high counts\n",
    "    else:\n",
    "        high_low.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed76cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"high_low\"] = high_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate the other attributes from the predicting attribute\n",
    "x = all_data.drop(\"Average Annual Count\",axis=1)\n",
    "x = x.drop(\"Age-Adjusted Incidence Rate([rate note]) - cases per 100,000\", axis = 1) #dropping extra predictive variables\n",
    "x = x.drop(\"Percent of Cases with Late Stage\", axis = 1)\n",
    "\n",
    "y = all_data[\"high_low\"]\n",
    "y = y.to_list()\n",
    "\n",
    "# y = target array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4347a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_acc(predictions, answers):\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for i in range(len(answers)):\n",
    "        if predictions[i] == answers[i]:\n",
    "            if predictions[i] == 1:\n",
    "                #If counts are high, and predicts high\n",
    "                TP += 1\n",
    "            else:\n",
    "                #If counts are low and predicts low\n",
    "                TN += 1\n",
    "        elif answers[i] == 1 and predictions[i] == 0:\n",
    "            #if counts are high but predicts low\n",
    "            FN += 1\n",
    "        else: \n",
    "            #if counts are low but predicts high\n",
    "            FP += 1 \n",
    "    return (TP, TN, FP, FN)\n",
    "\n",
    "def precision(TP, FP):\n",
    "    if TP + FP != 0:\n",
    "        return TP /(TP + FP)\n",
    "    return 0\n",
    "def recall(TP, FN):\n",
    "    if TP + FN != 0:\n",
    "        return TP / (TP + FN)\n",
    "    return 0\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall != 0:\n",
    "        return (2*precision*recall)/(precision + recall)\n",
    "    return 0\n",
    "def accuracy(TP, TN, FP, FN):\n",
    "    if TP + TN + FP + FN != 0:\n",
    "        return (TP + TN)/(TP + TN + FP + FN)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "# test_answers = []\n",
    "# train = []\n",
    "# train_answers = []\n",
    "\n",
    "# for i in range(len(x)):\n",
    "#     if i % 2 == 0:\n",
    "#         test.append(x[i])\n",
    "#         test_answers.append(y[i])\n",
    "#     else:\n",
    "#         train.append(x[i])\n",
    "#         train_answers.append(y[i])\n",
    "        \n",
    "# model2 = BernoulliNB()\n",
    "# model2.fit(train, train_answers) \n",
    "\n",
    "# train_predictions = model2.predict(train)\n",
    "# test_predictions = model2.predict(test)\n",
    "\n",
    "# TP_train, TN_train, FP_train, FN_train = response_acc(train_predictions, train_answers)\n",
    "# TP_test, TN_test, FP_test, FN_test = response_acc(test_predictions, test_answers)\n",
    "\n",
    "# train_accuracy = accuracy(TP_train, TN_train, FP_train, FN_train)\n",
    "# test_accuracy = accuracy(TP_test, TN_test, FP_test, FN_test)\n",
    "\n",
    "# train_f1 = f1_score(precision(TP_train, FP_train), recall(TP_train, FN_train))\n",
    "# test_f1 = f1_score(precision(TP_test, FP_test), recall(TP_test, FN_test))\n",
    "\n",
    "# print(\"Training accuracy:\", train_accuracy)\n",
    "# print(\"Test accuracy:\", test_accuracy)\n",
    "# print(\"Training f1 score:\", train_f1)\n",
    "# print(\"Test f1 score:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f507b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(x_train, y_train) \n",
    "\n",
    "train_predictions = model2.predict(x_train)\n",
    "test_predictions = model2.predict(x_test)\n",
    "\n",
    "TP_train, TN_train, FP_train, FN_train = response_acc(train_predictions, train_answers)\n",
    "TP_test, TN_test, FP_test, FN_test = response_acc(test_predictions, test_answers)\n",
    "\n",
    "train_accuracy = accuracy(TP_train, TN_train, FP_train, FN_train)\n",
    "test_accuracy = accuracy(TP_test, TN_test, FP_test, FN_test)\n",
    "\n",
    "train_f1 = f1_score(precision(TP_train, FP_train), recall(TP_train, FN_train))\n",
    "test_f1 = f1_score(precision(TP_test, FP_test), recall(TP_test, FN_test))\n",
    "\n",
    "print(\"Training accuracy:\", train_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)\n",
    "print(\"Training f1 score:\", train_f1)\n",
    "print(\"Test f1 score:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7014a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing Kfold over 10 folds\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "model = BernoulliNB()\n",
    "\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    X_train = np.array(x)[train_index]\n",
    "    X_test =  np.array(x)[test_index]\n",
    "    y_train = np.array(y)[train_index]\n",
    "    y_test = np.array(y)[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    test_predictions = model.predict(X_test) #outputs a vector of y's\n",
    "    train_predictions = model.predict(X_train)\n",
    "    \n",
    "    TP_train, TN_train, FP_train, FN_train = response_acc(train_predictions, y_train)\n",
    "    TP_test, TN_test, FP_test, FN_test = response_acc(test_predictions, y_test)\n",
    "    \n",
    "    training_accuracies.append(accuracy(TP_train, TN_train, FP_train, FN_train))\n",
    "    testing_accuracies.append(accuracy(TP_test, TN_test, FP_test, FN_test))\n",
    "\n",
    "    \n",
    "print(training_accuracies)\n",
    "print(testing_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea4f26",
   "metadata": {},
   "source": [
    "## it appears as though our data is not valuable for predicting high or low rates of breast cancer... lets try visualizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ab1f2",
   "metadata": {},
   "source": [
    "### Distribution of Annual Average Late stage breast cancer incidences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae32cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for i in range(40):\n",
    "    colors.append(i / 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c75c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize = (20,6))\n",
    "plt.hist(annual_counts, bins = 40, color = colors)\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"Number of Counties\", fontsize = 20)\n",
    "plt.ylabel(\"Frequency of Cases (log)\", fontsize = 20)\n",
    "plt.title(\"Distribution of Late Stage Breast Cancer Cases in US Counties\", fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a63c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
